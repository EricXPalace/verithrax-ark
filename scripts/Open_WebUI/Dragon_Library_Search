import sys
import subprocess
import importlib


# --- Self-repair dependency zone ---
def install_package(package):
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])
    except Exception as e:
        print(f"Failed to install {package}: {e}")


# Check and install BeautifulSoup4
try:
    import bs4
except ImportError:
    install_package("beautifulsoup4")
    import bs4

# Import standard library (Python 3 correct way)
import requests
import urllib.parse
from bs4 import BeautifulSoup


class Tools:
    def __init__(self):
        # Ensure host.docker.internal is used
        self.kiwix_url = "http://host.docker.internal:8081"

    def search_library(self, query: str) -> str:
        """
        Search the local Kiwix offline library using BeautifulSoup for precision.

        CRITICAL INSTRUCTION FOR AI:
        1. TRANSLATE user queries to ENGLISH.
        2. BE SPECIFIC. Include the technology name to avoid ambiguity.
           - Bad: "Container"
           - Good: "Docker Container"
           - Bad: "Match"
           - Good: "Rust Match pattern" or "Regex Match"

        :param query: The specific search term in English.
        """
        try:
            # 1. Construct search URL
            # Using content parameter can specify a specific ZIM, but currently Kiwix cross-library search is more convenient
            # We rely on Gemini to provide precise keywords (e.g. "Docker Volume")
            encoded_query = urllib.parse.quote(query)
            search_url = f"{self.kiwix_url}/search?pattern={encoded_query}"

            print(f"Searching: {search_url}")
            response = requests.get(search_url, timeout=10)

            if response.status_code != 200:
                return f"Library Error: Status {response.status_code}"

            # 2. Parse search results (using BS4)
            soup = BeautifulSoup(response.content, "html.parser")

            # Kiwix search results are usually located in the id="result_list" ul
            # We find the first 'meaningful' link
            results = soup.select("#result_list li a")

            target_link = None

            # Iterate through the first 3 results, excluding some system pages (as needed)
            for link in results[:3]:
                href = link.get("href")
                # Simple filtering: usually article paths contain /A/ (Article) or /I/ (Image - exclude)
                # We simply grab the first one here, because we believe the Prompt will become more accurate
                if href:
                    target_link = href
                    break

            if not target_link:
                return f"No results found for '{query}'. Try adding the technology name (e.g., 'Python list' instead of 'list')."

            # Handle relative path
            if not target_link.startswith("http"):
                if not target_link.startswith("/"):
                    target_link = "/" + target_link
                full_url = f"{self.kiwix_url}{target_link}"
            else:
                full_url = target_link

            # 3. Fetch article content
            article_resp = requests.get(full_url, timeout=10)
            article_soup = BeautifulSoup(article_resp.content, "html.parser")

            # 4. Clean content (remove noise)
            # Remove navigation, scripts, styles
            for element in article_soup(
                ["script", "style", "nav", "footer", "header", ".mw-jump-link"]
            ):
                element.decompose()

            text = article_soup.get_text(separator=" ", strip=True)

            # 5. Smart Truncation
            # To save tokens, we only return the first 6000 characters and include the source
            return f"Source: {full_url}\n\nDocumentation Content:\n{text[:6000]}..."

        except Exception as e:
            return f"Library Access Error: {e}"
